// Copyright 2022 Cerebras Systems.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// This example computes |b-A*x|_inf on a 2-by-2 rectangle which has P0.0, P0.1, P1.0 and P1.1
// The matrix A is distributed to every PE via MEMCPYH2D_DATA_1
// The vector x is distributed to first row PEs via MEMCPYH2D_DATA_2
// The vector b is distributed to first column PEs via MEMCPYH2D_DATA_3
// P1.0 sends out the result |b-A*x| via MEMCPYD2H_DATA_1
//
// Each PE receives a vector x and computes A*x locally, then performs a row reduction to finish y = b - A*x
// The last column contains the vector y, and performs a column reduction to compute |b-A*x|

// Notation: a PE (Px.y) is labeled as (px = x, py = y)
param memcpyParams: comptime_struct;

param MEMCPYH2D_DATA_1: color; // 1st H2D: A
param MEMCPYH2D_DATA_2: color; // 1st H2D: x
param MEMCPYH2D_DATA_3: color; // 1st H2D: b

param MEMCPYD2H_DATA_1: color; // 1st D2H: nrm
                               // P1.0: send |b-A*x| to the fabric

// local colors
param RXACT_X: color; // py = 0: don't care
                       // py > 0: receive vector x from the north
param TXACT_X: color; // py = 0: send x to the south
param RXACT_Y: color; // px = 0: forward b-A*x to the east
                       // px = 1: receive partial sum (b - A*x) from px = 0
param TXACT_Y: color;   // px = 0: send parital sum to px = 1
param RXACT_NRM: color; // P1.0: receive nrm from P1.1
param TXACT_NRM: color; // P1.1: send local nrm to P1.0

// local tasks
param COMP: color;     // compute local Ax = A*x
param REDUCE: color;   // compute either local b - A*x or local y - A*x
param DONE: color;     // compute |b-A*x| and send out the result

// (_px, _py) is logical coordinate of a PE in a region of interest
param _px : i16 ; // coordinate x in ROI
param _py : i16 ; // coordinate y in ROI

param LOCAL_OUT_SZ:i16 ;  // dimension of submatrix A is LOCAL_OUT_SZ-by-LOCAL_IN_SZ
                          // dimension of subvector y is LOCAL_OUT_SZ-by-1
param LOCAL_IN_SZ:i16  ;  // dimension of subvector x is LOCAL_IN_SZ-by-1


const sys_mod = @import_module( "<memcpy/memcpy>", @concat_structs(memcpyParams, .{
     .MEMCPYH2D_1=MEMCPYH2D_DATA_1,
     .MEMCPYH2D_2=MEMCPYH2D_DATA_2,
     .MEMCPYH2D_3=MEMCPYH2D_DATA_3,

     .MEMCPYD2H_1=MEMCPYD2H_DATA_1,

    }));


////////////////////////////////////////////////////////////////////////////////
// Main memory (48KB)
////////////////////////////////////////////////////////////////////////////////

// A is LOCAL_OUT_SZ-by-LOCAL_IN_SZ with lda=LOCAL_OUT_SZ
// x is LOCAL_IN_SZ-by-1
// y is LOCAL_OUT_SZ-by-1

// Assumption
// - _MAX_SIZE_X >= LOCAL_IN_SZ
// - _MAX_SIZE_Y >= LOCAL_OUT_SZ
// - _MAX_SIZE_A >= LOCAL_OUT_SZ*LOCAL_IN_SZ
const  _MAX_SIZE_A = LOCAL_OUT_SZ * LOCAL_IN_SZ;
const  _MAX_SIZE_X = LOCAL_IN_SZ;
const  _MAX_SIZE_Y = LOCAL_OUT_SZ;

var A  = @zeros([_MAX_SIZE_A]f32);
var x  = @zeros([_MAX_SIZE_X]f32);
var Ax = @zeros([_MAX_SIZE_Y]f32);
var y  = @zeros([_MAX_SIZE_Y]f32);

// workspace for outer-product version of GEMV
var ws = @zeros([_MAX_SIZE_Y]f32);

var nrm = @zeros([1]f32);
var local_nrm : f32 = @as(f32, 0.0);

var idx_A: i16 = 0;
var idx_x: i16 = 0;
var idx_b: i16 = 0;

var A_is_ready: bool = false;
var x_is_ready: bool = false;
var b_is_ready: bool = false;
var Ax_is_ready: bool = false;

////////////////////////////////////////////////////////////////////////////////
// DSDs
// data-structure descriptors (DSDs), loaded into data-structure registers (DSRs) to configure DSR
// The DSDs are typically put in their own data segment that is placed right above lo-mem.?
//
// The content of a DSR is a DSD, which is a data structure stored in memory.
// A DSR is a numbered hardware register and, like a GPR, is memory mapped.
// DSRs hold DSDs. Their numbers are stored in instruction operand fields, where the DSD held by the DSR
// serves to describe the actual data operand, which is a memory or fabric tensor.
////////////////////////////////////////////////////////////////////////////////

const mem_x_buf_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{LOCAL_IN_SZ} -> x[i] });

const fab_recv_x_wdsd = @get_dsd(fabin_dsd, .{
    .extent = LOCAL_IN_SZ,
    .fabric_color = RXACT_X,
    .input_queue = 0
});

const fab_trans_x_wdsd = @get_dsd(fabout_dsd, .{
    .extent = LOCAL_IN_SZ,
    .fabric_color = TXACT_X,
    .output_queue = 1
});

const mem_y_buf_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{LOCAL_OUT_SZ} -> y[i] });

const fab_recv_y_wdsd = @get_dsd(fabin_dsd, .{
    .extent = LOCAL_OUT_SZ,
    .fabric_color = RXACT_Y,
    .input_queue = 1
});


const fab_trans_psum_wdsd = @get_dsd(fabout_dsd, .{
    .extent = LOCAL_OUT_SZ,
    .fabric_color = TXACT_Y,
    .output_queue = 0
});


const mem_nrm_buf_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{1} -> nrm[i] });

const fab_recv_nrm_wdsd = @get_dsd(fabin_dsd, .{
    .extent = 1,
    .fabric_color = RXACT_NRM,
    .input_queue = 2
});


// only used in P1.1, send the partial nrm to P1.0
const fab_trans_nrm_wdsd_p11 = @get_dsd(fabout_dsd, .{
    .extent = 1,
    .fabric_color = TXACT_NRM,
    .output_queue = 1
});


// only used in P1.0: send the result |b-A*x| via D2H
const fab_trans_nrm_wdsd_p10 = @get_dsd(fabout_dsd, .{
    .extent = 1,
    .fabric_color = MEMCPYD2H_DATA_1,
    .output_queue = 1
});


////////////////////////////////////////////////////////////////////////////////
// Tasks
////////////////////////////////////////////////////////////////////////////////

const gemv_mod = @import_module( "gemv.csl" , .{  .sizeA = _MAX_SIZE_A, .sizeX = _MAX_SIZE_X, .sizeY = _MAX_SIZE_Y });

const axpy_mod = @import_module( "axpy.csl" , .{  .sizeXY = _MAX_SIZE_Y });

const nrminf_mod = @import_module( "nrminf.csl" , .{  .sizeX = _MAX_SIZE_Y });


// All PEs compute local A*x after x is received
task f_comp() void {

  //  Ax = A * x + 0*y
  var alpha: f32 = @as(f32, 1.0);
  var beta : f32 = @as(f32, 0.0);
  gemv_mod.sgemv_outer(LOCAL_OUT_SZ, LOCAL_IN_SZ, alpha, &A, LOCAL_OUT_SZ, &x, beta, &Ax, &ws);

  Ax_is_ready = true;

  // px = 0: receive vector b from H2D_2
  // px = 1: receive partial sum from the west
  if (0 == _px){
    if (b_is_ready){
      // Ax and b are ready, compute |b-A*x|
      @activate(REDUCE);
    }
    // if b is not received, trigger REDUCE at f_memcpyh2d_data_3
  }else{
    @mov32(mem_y_buf_dsd, fab_recv_y_wdsd, .{.async=true, .activate = f_reduce});
  }
}


task f_reduce() void {
    // y  = b if px = 0 (from the west)
    //    = partial sum if px = 1 (from the west)
    // Ax = local gemv

    // px = 0: y = b - A*x
    // px = 1: y = y_recv - A*x, where y_recv = b-A*x in px=0
    var alpha : f32 =  @as(f32, -1.0);
    axpy_mod.saxpy( LOCAL_OUT_SZ, alpha, &Ax, &y);

    if (0 == _px){
        // send partial sum to the east and finish
        @mov32(fab_trans_psum_wdsd, mem_y_buf_dsd, .{.async=true});
    }else{
        // px = 1: compute norm of local (b-A*x)
        nrminf_mod.snrminf(LOCAL_OUT_SZ, &y, &local_nrm);

        if (0 == _py){
            // P1.0: receive nrm from the south
            @mov32(mem_nrm_buf_dsd, fab_recv_nrm_wdsd, .{.async=true, .activate = f_done});
        }else{
            // P1.1: send local nrm to north and finish
            // only DSD operands are allowed for an async operation, so remove ".async=true"
            @fmovs(fab_trans_nrm_wdsd_p11, local_nrm);
        }
    }
}


// Only P1.0 triggers f_done to send out |b-A*x|
task f_done() void {

  // loc_nrm = |b - A*x| locally
  // nrm[0] = |b - A*x| from south
  // nrm[0] = max( nrm[0], local_nrm)
  if (nrm[0] < local_nrm){
    nrm[0] = local_nrm;
  }

  // send |b-A*x| via D2H
  @mov32(fab_trans_nrm_wdsd_p10, mem_nrm_buf_dsd, .{.async=true});
}


// Every PE reads A[LOCAL_OUT_SZ * LOCAL_IN_SZ] first
//
// py = 0 may read x[LOCAL_IN_SZ] before A, however
// it only receives x in the memory, only A can trigger COMP
//
task f_memcpyh2d_data_1( data : f32 ) void {
  A[idx_A] = data;
  idx_A += 1;
  if ( idx_A == (LOCAL_OUT_SZ * LOCAL_IN_SZ) ){
    A_is_ready = true;
    if (0 == _py){
      if (x_is_ready){
        // A and x are ready, compute A*x
        @activate(COMP);
      }
      // if x is not ready, then trigger COMP when x is received in f_memcpyh2d_data_2
    }else{
      // 0 < _py: receive x from north
      @mov32(mem_x_buf_dsd, fab_recv_x_wdsd, .{.async=true, .activate = f_comp});
    }
  }
}


// py = 0: Pj.0 reads xj
task f_memcpyh2d_data_2( data : f32 ) void {
  x[idx_x] = data;
  idx_x += 1;
  if (idx_x == LOCAL_IN_SZ){
    x_is_ready = true;
    // vector x is received, forwards x to south
    if (0 == _py){
      @mov32(fab_trans_x_wdsd, mem_x_buf_dsd, .{.async=true});
    }
    if (A_is_ready){
      @activate(COMP);
    }
    // if A is not received yet, trigger COMP at f_memcpyh2d_data_1
  }
}


// px = 0: P0,j reads bj
task f_memcpyh2d_data_3( data : f32 ) void {
  y[idx_b] = data;
  idx_b += 1;
  if (idx_b == LOCAL_OUT_SZ){
    b_is_ready = true;
    if (Ax_is_ready){
      // Ax, b are received, compute |b-A*x|
      @activate(REDUCE);
    }
  }
}

comptime {

    // use microthread to read x, b or partial sum y/nrm, so block RXACT_X, RXACT_Y and RXACT_NRM
    @block(RXACT_X);
    @block(RXACT_Y);
    @block(RXACT_NRM);

    @bind_task(f_comp,    COMP);
    @bind_task(f_reduce,  REDUCE);
    @bind_task(f_done,    DONE);
    @bind_task(f_memcpyh2d_data_1, MEMCPYH2D_DATA_1 ); // read A
    @bind_task(f_memcpyh2d_data_2, MEMCPYH2D_DATA_2 ); // read x
    @bind_task(f_memcpyh2d_data_3, MEMCPYH2D_DATA_3 ); // read b
}
